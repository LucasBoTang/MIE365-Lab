{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 11 Average Markov Decision Processes\n",
        "\n",
        "Made & Presented by Bo Tang\n",
        "\n",
        "In this lab, we will explore the concept of Markov Decision Processes (MDPs) with average reward in the infinite-horizon setting. Unlike the discounted case where future rewards are geometrically devalued, the average reward framework focuses on optimizing the long-run average cost per period. We will formulate the problem as a Linear Program (LP) and implement it using Gurobi."
      ],
      "metadata": {
        "id": "GL4-o2uwpsV2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1MsBl6Fprzk",
        "outputId": "43f3bc46-9a35-472f-fd5a-49bfdfa27eb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gurobipy\n",
            "  Downloading gurobipy-12.0.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (16 kB)\n",
            "Downloading gurobipy-12.0.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (14.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gurobipy\n",
            "Successfully installed gurobipy-12.0.1\n"
          ]
        }
      ],
      "source": [
        "# install gurobipy first\n",
        "! pip install gurobipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gurobipy as gp\n",
        "import numpy as np\n",
        "from gurobipy import GRB"
      ],
      "metadata": {
        "id": "0nA9zneOqMP4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1 Markov Decision Processes Review**\n",
        "\n",
        "We begin by reviewing the standard framework of Markov Decision Processes (MDPs), which provide a mathematical model for sequential decision-making under uncertainty.\n",
        "\n",
        "A typical MDP consists of:\n",
        "- A finite set of states $S$\n",
        "- For each state $s \\in S$, a finite set of actions $A(s)$ available in that state\n",
        "- A reward function $r(s, a)$ that gives the immediate reward for taking action $a$ in state $s$\n",
        "- A transition probability function $P(t \\mid s, a)$, representing the probability of moving to state $t$ after taking action $a$ in state $s$\n",
        "- A decision policy $\\pi_s$, which specifies the action to take in each state\n",
        "\n",
        "The goal is to find a policy $\\pi$ that maximizes some measure of long-term reward — typically, either the expected total discounted reward or the average reward per time step."
      ],
      "metadata": {
        "id": "TgDrDh63qRDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2 Markov Chain and Steady-State Distribution**\n",
        "\n",
        "Once an MDP is governed by a fixed (stationary) policy, the evolution of the system becomes a **Markov Chain**. In such a chain:\n",
        "\n",
        "- The system can be in one of $n$ possible states.\n",
        "- The **transition probability** from state $i$ to $j$ is denoted $p_{ij}$.\n",
        "- The **steady-state distribution** $\\pi = (\\pi_1, \\ldots, \\pi_n)$ describes the long-run probability of being in each state.\n",
        "\n",
        "The defining condition for a steady-state distribution is:\n",
        "\n",
        "$$\n",
        "\\pi_j = \\sum_{i=1}^{n} \\pi_i \\cdot p_{ij}, \\quad \\text{for all } j\n",
        "$$\n",
        "\n",
        "This ensures that the distribution remains **unchanged over time**, i.e., $\\pi = \\pi P$."
      ],
      "metadata": {
        "id": "ZWE2jS3hzr1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3 Average Reward Formulation**\n",
        "\n",
        "We now shift our focus from discounted total reward to the long-run **average reward per time step**, which leads to a different modeling framework based on **steady-state analysis**.\n",
        "\n",
        "Instead of computing the present value of future rewards, we focus on the **long-run frequency** with which the system visits each state-action pair.\n",
        "\n",
        "In the MDP setting, we extend the idea from Markov Chains:  \n",
        "Each action $a$ taken in state $s$ leads to a different transition matrix $P_a$. We define $\\pi_{sa}$ as the **steady-state joint probability** that the system is in state $s$ and takes action $a$.\n",
        "\n",
        "Assuming the Markov Decision Process (MDP) is recurrent and a stationary policy induces a steady-state distribution, the **average reward** can be expressed as:\n",
        "\n",
        "$$\n",
        "\\bar{r} = \\sum_{s \\in S} \\sum_{a \\in A(s)} r(s, a) \\cdot \\pi_{sa}\n",
        "$$\n",
        "\n",
        "In other words, instead of computing a value function for each state, we now optimize over the **frequencies** of visiting each state-action pair. The total average reward becomes a **weighted sum of immediate rewards**, where the weights reflect how often each $(s, a)$ is used under the system's long-run behavior."
      ],
      "metadata": {
        "id": "inl9b7-XuIjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.1 Objective and Decision Variables**\n",
        "\n",
        "Given this setup, we formulate a linear program where the decision variable is the joint steady-state distribution $\\pi_{sa}$. The goal is to maximize the long-run average reward:\n",
        "\n",
        "$$\n",
        "\\max_{\\pi} \\sum_{s \\in S} \\sum_{a \\in A(s)} r(s, a) \\pi_{sa}\n",
        "$$\n",
        "\n",
        "However, in order for this optimization problem to be meaningful and well-defined, we must ensure that the variables $\\pi_{sa}$ represent a valid steady-state distribution. This requires several key constraints:\n",
        "\n",
        "##### **3.2 Flow Balance Constraints**\n",
        "\n",
        "The flow balance constraints ensure that, in steady-state, the total probability \"flowing into\" each state equals the total probability \"flowing out\". This reflects a stable long-run system where no probability mass accumulates or vanishes in any state.\n",
        "\n",
        "For every state $j \\in S$, the balance condition is:\n",
        "\n",
        "$$\n",
        "\\sum_{a \\in A(t)} \\pi_{ta} = \\sum_{s \\in S} \\sum_{a \\in A(s)} P(t \\mid s, a) \\cdot \\pi_{sa}\n",
        "$$\n",
        "\n",
        "- Left-hand side: The total probability of being in state $t$ and taking some action $a$.\n",
        "- Right-hand side: The total probability of transitioning into state $t$ from any state-action pair $(s, a)$.\n",
        "\n",
        "This constraint guarantees the internal consistency of the joint distribution $\\pi_{sa}$ under the system dynamics.\n",
        "\n",
        "##### **3.3 Probability Normalization Constraint**\n",
        "\n",
        "Since $\\pi_{sa}$ represents a joint probability distribution over state-action pairs, its total sum must equal 1:\n",
        "\n",
        "$$\n",
        "\\sum_{s \\in S} \\sum_{a \\in A(s)} \\pi_{sa} = 1\n",
        "$$\n",
        "\n",
        "This ensures that $\\pi_{sa}$ corresponds to a proper probability distribution.\n",
        "\n",
        "##### **3.4 Non-Negativity Constraints**\n",
        "\n",
        "All probabilities must be non-negative:\n",
        "\n",
        "$$\n",
        "\\pi_{sa} \\geq 0, \\quad \\forall s \\in S,\\ a \\in A(s)\n",
        "$$\n",
        "\n",
        "\n",
        "##### **3.5 Linear Programming Formulation**\n",
        "\n",
        "Together, the objective and these constraints form a complete linear program for solving average reward MDPs:\n",
        "\n",
        "$$\n",
        "\\max_{\\pi} \\sum_{s \\in S} \\sum_{a \\in A(s)} r(s, a) \\cdot \\pi_{sa}\n",
        "$$\n",
        "\n",
        "Subject to:\n",
        "\n",
        "$$\n",
        "\\sum_{a \\in A(t)} \\pi_{ta} = \\sum_{s \\in S} \\sum_{a \\in A(s)} P(t \\mid s, a) \\cdot \\pi_{sa}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sum_{s \\in S} \\sum_{a \\in A(s)} \\pi_{sa} = 1\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\pi_{sa} \\geq 0, \\quad \\forall s \\in S,\\ a \\in A(s)\n",
        "$$"
      ],
      "metadata": {
        "id": "3Yq2ywfW2cy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4 Problem Statement**\n",
        "\n",
        "A warehouse has an end-of-period capacity of 3 units. During a period in which production takes place, a setup cost of \\$4 is incurred. A \\$1 holding cost is assessed against each unit of a period’s ending inventory. Also, a variable production cost of $ 1 per unit is incurred. During each period demand is equally likely to be 1 or 2 units. All demand must be met on time. Minimize expected discounted costs over an infinite horizon.\n",
        "\n",
        "Try implementing this LP model:"
      ],
      "metadata": {
        "id": "LqOckko__1yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define parameters\n",
        "states = [0, 1, 2, 3] # states\n",
        "\n",
        "# define allowed actions per state\n",
        "actions = {\n",
        "    0: [2, 3, 4],\n",
        "    1: [1, 2, 3],\n",
        "    2: [0, 1, 2],\n",
        "    3: [0, 1]\n",
        "}\n",
        "\n",
        "# TODO: define transition probabilities P(t | s, a)\n",
        "P = {\n",
        "    # s = 0\n",
        "\n",
        "    # s = 1\n",
        "\n",
        "    # s = 2\n",
        "\n",
        "    # s = 3\n",
        "\n",
        "}\n",
        "\n",
        "# TODO: define the cost with s-a pairs\n",
        "cost = {\n",
        "    # s = 0\n",
        "\n",
        "    # s = 1\n",
        "\n",
        "    # s = 2\n",
        "\n",
        "    # s = 3\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "BhNcIkNhqNyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ceate a model\n",
        "m = gp.Model(\"MDP\")\n",
        "# varibles\n",
        "pi = m.addVars(cost, lb=0, name=\"steady-state distribution\")\n",
        "# TODO: obj func: minimize total expected cost\n",
        "m.setObjective\n",
        "# TODO: constraint\n",
        "m.addConstr\n",
        "# solve\n",
        "m.optimize()\n",
        "# Output results\n",
        "print(\"\\nOptimal steady-state policy (pi_sa > 0):\")\n",
        "for (s, a) in cost:\n",
        "    if pi[s, a].X > 1e-6:\n",
        "        print(f\"State {s}, Action {a} -> π = {pi[s, a].X:.4f}\")"
      ],
      "metadata": {
        "id": "XEId4OFOa08W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5WQrKQQPa1EB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}