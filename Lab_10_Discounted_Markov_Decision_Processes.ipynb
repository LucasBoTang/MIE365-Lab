{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 10 Discounted Markov Decision Processes\n",
        "\n",
        "Made & Presented by Bo Tang\n",
        "\n",
        "In this lab, we will explore the concept of Markov Decision Processes in the infinite horizon discounted case, understand how to formulate them, and implement solution methods including Linear Programming, Policy Iteration, and Value Iteration."
      ],
      "metadata": {
        "id": "o09r4esSYnGS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqdRck4MYgyG"
      },
      "outputs": [],
      "source": [
        "# install gurobipy first\n",
        "! pip install gurobipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gurobipy as gp\n",
        "import numpy as np\n",
        "from gurobipy import GRB"
      ],
      "metadata": {
        "id": "dtFVZ354ZJo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1 Dynamic Programming Review**\n",
        "\n",
        "In Dynamic Programming (DP), the problem is divided into several stages, each with:\n",
        "- **Stages** $n = 1, 2, \\cdots, N$: A sequence of decision points.\n",
        "- **State Variables** $s_n$: Describe the current state, containing all necessary information to make the next decision.\n",
        "- **Decision Variables** $x_n$: The choices available at each stage, which affect the next state.\n",
        "- **Transition Relations** $s_{n+1} = g_n(s_n, x_n)$: Define how the system moves from one state to another based on the decision made.\n",
        "- **Immediate Return** $h_n(s_n, x_n)$: The reward or cost associated with each decision at each stage.\n",
        "\n",
        "The solution to a dynamic programming problem typically involves **backward recursion**, where we start from the last stage and move backward to calculate the optimal solution. With these basic concepts, we can express the optimal return from stage $n$ to the end of the horizon as:\n",
        "$$\n",
        "f_n(s_n) = \\max_{x_n} \\big[ h_n(s_n, x_n) + f_{n+1}\\big(g_n(s_n, x_n)\\big) \\big]\n",
        "$$"
      ],
      "metadata": {
        "id": "qB6Q11ODaqnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2 From DP to MDP**\n",
        "\n",
        "#### **2.1 Discount with Infinite Horizon**\n",
        "\n",
        "In previous DP problems, we worked with multiple stages, but those stages $s$ were not always interpreted as time periods. In the context of MDPs, we now make a key assumption: each stage represents a **time period** $t$, and here the process continues **indefinitely**.\n",
        "\n",
        "For instance, consider a company that expects to operate for the foreseeable future without a predefined endpoint. In such settings, we model the system over an infinite horizon and incorporate a **discount factor** $\\beta \\in (0, 1)$ to prioritize immediate costs or rewards over distant ones.\n",
        "\n",
        "Under these assumptions, the optimal value function $f_t(s_t)$ for each state $s_t$ satisfies the Bellman equation:\n",
        "\n",
        "$$\n",
        "f_t(s_t) = \\max_{x_t} \\big[ h_t(s_t, x_t) + \\beta \\cdot f_{t+1}\\big(g_t(s_t, x_t)\\big) \\big]\n",
        "$$\n",
        "\n",
        "This recursive equation expresses the optimal total return starting from state $s_t$, as the best immediate reward plus the discounted future value.\n",
        "\n",
        "#### **2.2 Stochastic Transitions**\n",
        "\n",
        "So far, in both classical Dynamic Programming and our previous formulations, we assumed deterministic transitions—that is, given the current state $s_t$ and action $x_t$, the next state $s_{t+1}$ is fully determined by a function $g_t(s_t, x_t)$.\n",
        "\n",
        "However, in many real-world settings, transitions are inherently stochastic. That is, after taking action $x_t$ in state $s_t$, the next state $s_{t+1}$ is not deterministic, but follows a probability distribution:\n",
        "\n",
        "$$\n",
        "\\text{Pr}(s_{t+1} \\mid s_t, x_t)\n",
        "$$\n",
        "\n",
        "To incorporate this uncertainty, we modify the Bellman equation to take the expected value over all possible next states. The value function now becomes:\n",
        "\n",
        "$$\n",
        "f_t(s_t) = \\max_{x_t} \\left[ h_t(s_t, x_t) + \\beta \\cdot \\sum_{s_{t+1}} \\Pr(s_{t+1} \\mid s_t, x_t) \\cdot f_{t+1}(s_{t+1}) \\right]\n",
        "$$\n",
        "\n",
        "This expression reflects the fact that, rather than knowing exactly what happens next, we optimize based on the expected return across all possible outcomes weighted by their transition probabilities.\n",
        "\n",
        "#### **2.3 Notation**\n",
        "\n",
        "To simplify notation in the infinite-horizon case, we drop the explicit time index $t$, and assume the system is stationary—that is, transition probabilities and rewards do not change over time.\n",
        "\n",
        "- Let $s$ denote the current state\n",
        "- Let $a$ denote an action taken in state $s$, where $a \\in A(s)$ and $A(s)$ is the set of actions feasible from state $s$\n",
        "- Let $r(s, a)$ be the immediate cost (or negative reward)\n",
        "- Let $P(s' \\mid s, a)$ be the probability of transitioning to state $s'$ after taking action $a$ in state $s$\n",
        "- Let $\\beta \\in (0, 1)$ be the discount factor\n",
        "- Let $V(s)$ be the value function, representing the expected discounted cost starting from state $s$\n",
        "\n",
        "With this notation, the Bellman equation becomes:\n",
        "\n",
        "$$\n",
        "V(s) = \\min_{a \\in A(s)} \\left[ r(s, a) + \\beta \\cdot \\sum_{s'} P(s' \\mid s, a) \\cdot V(s') \\right]\n",
        "$$"
      ],
      "metadata": {
        "id": "QgZWQqu8bcSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3 Problem Statement**\n",
        "\n",
        "A warehouse has an end-of-period capacity of 3 units. During a period in which production takes place, a setup cost of \\$4 is incurred. A \\$1 holding cost is assessed against each unit of a period’s ending inventory. Also, a variable production cost of $ 1 per unit is incurred. During each period demand is equally likely to be 1 or 2 units. All demand must be met on time, and β = 0.8. Minimize expected discounted costs over an infinite horizon.\n",
        "\n",
        "##### Question:\n",
        "To formulate this problem as a dynamic program or MDP, identify the following elements:\n",
        "- Stages:\n",
        "- States:\n",
        "- Decisions:"
      ],
      "metadata": {
        "id": "Dz7pC5aNi0r7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# states\n",
        "states = [0, 1, 2, 3]\n",
        "# discounted factor\n",
        "beta = 0.8"
      ],
      "metadata": {
        "id": "XuI9pvSGtnky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1 Linear Programming**\n",
        "\n",
        "One way to solve an infinite-horizon discounted MDP is to formulate it as a linear program (LP). This approach is based on the Bellman equations.\n",
        "\n",
        "For each state $s$, we introduce a variable $V(s)$ representing the expected discounted cost starting from state $s$ for all possible action $a$.\n",
        "$$\n",
        "V(s) = \\min_{a \\in A(s)} \\left[ r(s, a) + \\beta \\cdot \\sum_{s'} P(s' \\mid s, a) \\cdot V(s') \\right]\n",
        "$$\n",
        "\n",
        "Then, we transform the equation for each possible action $a$ in each state $s$, ensuring that the Bellman inequality holds:\n",
        "\n",
        "$$\n",
        "V(s) \\leq r(s, a) + \\beta \\cdot \\sum_{s'} P(s' \\mid s, a) \\cdot V(s') \\quad \\forall a \\in A(s)\n",
        "$$\n",
        "\n",
        "To complete the linear program, we define the **objective function** as:\n",
        "\n",
        "$$\n",
        "\\max \\sum_s V(s)\n",
        "$$\n",
        "\n",
        "This objective ensures that the inequalities are **tight** for at least one action in each state—that is, the Bellman inequality becomes an equality for the optimal action. We can recover the optimal policy once we solve the LP and obtain the value function $V(s)$.\n"
      ],
      "metadata": {
        "id": "bsSb3qiYkjKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try implementing this LP model:"
      ],
      "metadata": {
        "id": "1oMR6cDrsk72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ceate a model\n",
        "m = gp.Model(\"MDP\")\n",
        "# varibles\n",
        "v = m.addVars(states, lb=-GRB.INFINITY, name=\"value\") # value\n",
        "# TODO: obj func\n",
        "m.setObjective\n",
        "# TODO: constr\n",
        "# v0 = min x in {2,3,4}\n",
        "m.addConstr\n",
        "# v1 = min x in {1,2,3}\n",
        "m.addConstr\n",
        "# v2 = min x in {0,1,2}\n",
        "m.addConstr\n",
        "# v3 = min x in {0,1}\n",
        "m.addConstr\n",
        "# solves\n",
        "m.optimize()\n",
        "# value\n",
        "print(\"Model Solution:\")\n",
        "for s in states:\n",
        "    print(\"v_{} = {:.2f}\".format(s, v[s].x), end=\" \")"
      ],
      "metadata": {
        "id": "rH3gbAduapbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question:\n",
        "What is the actions for the best policy?\n",
        "\n",
        "$x^*(0)=4, x^*(1)=3, x^*(2)=0, x^*(3)=0$"
      ],
      "metadata": {
        "id": "LbwqsNXSB5jY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.2 Policy Iteration**\n",
        "\n",
        "This method consists of two main steps, which are repeated until convergence:\n",
        "\n",
        "1. **Policy Evaluation**:  \n",
        "   Given a fixed policy $\\pi$, compute the value function $V^\\pi(s)$ for all states $s$.  \n",
        "   This involves solving a system of linear equations:\n",
        "\n",
        "   $$\n",
        "   V^\\pi(s) = r(s, \\pi(s)) + \\beta \\cdot \\sum_{s'} P(s' \\mid s, \\pi(s)) \\cdot V^\\pi(s') \\quad \\forall s\n",
        "   $$\n",
        "\n",
        "2. **Policy Improvement**:  \n",
        "   Given the current value function $V^\\pi$, update the policy by choosing the best action:\n",
        "\n",
        "   $$\n",
        "   \\pi_{\\text{new}}(s) = \\arg\\min_{a \\in A(s)} \\left[ r(s, a) + \\beta \\cdot \\sum_{s'} P(s' \\mid s, a) \\cdot V^\\pi(s') \\right]\n",
        "   $$\n",
        "\n",
        "Start with an initial policy $\\pi$ and repeat these two steps until the policy stops changing. At that point, we have found the **optimal policy**."
      ],
      "metadata": {
        "id": "TZ4mMpz2-3DV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start from this initial policy:"
      ],
      "metadata": {
        "id": "UCq2juKRCqC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_policy = {\n",
        "    0: 2,   # min([2, 3, 4])\n",
        "    1: 1,   # min([1, 2, 3])\n",
        "    2: 0,   # min([0, 1, 2])\n",
        "    3: 0    # min([0, 1])\n",
        "}"
      ],
      "metadata": {
        "id": "piHkgyRUCv1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question:\n",
        "Why is it reasonable to initialize our policy in this way?\n",
        "\n",
        "Because it choose the smallest action in each state."
      ],
      "metadata": {
        "id": "3J0rIM1zCuyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try implementing two iteration of this algorithm by writing:\n",
        "- Evaluate a given policy (solve a linear system)\n",
        "- Improve the policy based on the current value function"
      ],
      "metadata": {
        "id": "RVrEpsPaAfRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first interation\n",
        "print(\"Iteration 1:\")\n",
        "\n",
        "# evaluation\n",
        "m = gp.Model(\"Eval\")\n",
        "# turn off output log\n",
        "m.setParam(\"OutputFlag\", 0)\n",
        "# varibles\n",
        "v = m.addVars(states, lb=-GRB.INFINITY, name=\"value\")\n",
        "# linear equations\n",
        "m.addConstr(v[0] == 6 + beta * (0.5 * v[0] + 0.5 * v[1]))\n",
        "m.addConstr(v[1] == 6 + beta * (0.5 * v[0] + 0.5 * v[1]))\n",
        "m.addConstr(v[2] == 2 + beta * (0.5 * v[0] + 0.5 * v[1]))\n",
        "m.addConstr(v[3] == 3 + beta * (0.5 * v[1] + 0.5 * v[2]))\n",
        "# dummy objective (we just want feasibility)\n",
        "m.setObjective(0, GRB.MINIMIZE)\n",
        "# solve\n",
        "m.optimize()\n",
        "V = {s: v[s].X for s in states}\n",
        "print(\"\\nValue after first evaluation:\", {k: round(v, 2) for k, v in V.items()})\n",
        "\n",
        "# improvement\n",
        "policy = {}\n",
        "# state = 0 and a in (2, 3, 4)\n",
        "pi0 = {2: 6 + beta * (0.5 * V[0] + 0.5 * V[1]),\n",
        "       3: 7 + beta * (0.5 * V[1] + 0.5 * V[2]),\n",
        "       4: 8 + beta * (0.5 * V[2] + 0.5 * V[3])}\n",
        "policy[0] = min(pi0, key=pi0.get)\n",
        "# state = 1 and a in (1, 2, 3)\n",
        "pi1 = {1: 6 + beta * (0.5 * V[0] + 0.5 * V[1]),\n",
        "       2: 7 + beta * (0.5 * V[1] + 0.5 * V[2]),\n",
        "       3: 8 + beta * (0.5 * V[2] + 0.5 * V[3])}\n",
        "policy[1] = min(pi1, key=pi1.get)\n",
        "# state = 2 and a in (0, 1, 2)\n",
        "pi2 = {0: 2 + beta * (0.5 * V[0] + 0.5 * V[1]),\n",
        "       1: 7 + beta * (0.5 * V[1] + 0.5 * V[2]),\n",
        "       2: 8 + beta * (0.5 * V[2] + 0.5 * V[3])}\n",
        "policy[2] = min(pi2, key=pi2.get)\n",
        "# state = 3 and a in (0, 1)\n",
        "pi3 = {0: 3 + beta * (0.5 * V[1] + 0.5 * V[2]),\n",
        "       1: 8 + beta * (0.5 * V[2] + 0.5 * V[3])}\n",
        "policy[3] = min(pi3, key=pi3.get)\n",
        "print(\"\\nPolicy after first improvement:\", {k: round(v, 2) for k, v in policy.items()})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfvlUCNa-3K-",
        "outputId": "86015c49-32ff-435d-c656-20a368f44eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1:\n",
            "\n",
            "Value after first evaluation: {0: 30.0, 1: 30.0, 2: 26.0, 3: 25.4}\n",
            "\n",
            "Policy after first improvement: {0: 4, 1: 3, 2: 0, 3: 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# second iteration\n",
        "print(\"Iteration 2:\")\n",
        "\n",
        "# TODO: evaluation\n",
        "\n",
        "# TODO: improvement"
      ],
      "metadata": {
        "id": "wL_l5UW0c7YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question:\n",
        "Has policy converged?"
      ],
      "metadata": {
        "id": "LVDASoJUks3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.3 Value Iteration**\n",
        "\n",
        "Value iteration directly updates the value function iteratively based on the Bellman equations.\n",
        "\n",
        "We start from an initial guess for the value function—**often all zeros**—and repeatedly update each state's value using:\n",
        "\n",
        "$$\n",
        "V_{k+1}(s) = \\min_{a \\in A(s)} \\left[ r(s, a) + \\beta \\sum_{s'} P(s' \\mid s, a) \\cdot V_k(s') \\right]\n",
        "$$\n",
        "\n",
        "This recursive process continues until the value function converges, that is, until the maximum change in value across all states is below a predefined small value $\\epsilon$.\n",
        "\n",
        "nce the value function has converged, the optimal policy can be recovered by selecting, for each state, the action that minimizes the right-hand side of the Bellman equation."
      ],
      "metadata": {
        "id": "yeB1NIullxj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try implementing value iteration starting from $V(s) = 0$ with 100 iterations:"
      ],
      "metadata": {
        "id": "_PSBVZrGmxfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# init\n",
        "V = {0:0, 1:0, 2:0, 3:0}"
      ],
      "metadata": {
        "id": "jNOvW3H4bRVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valueIterationUpdate(V, beta):\n",
        "    V_new = {}\n",
        "    # TODO: Update values\n",
        "    # state = 0 and a in (2, 3, 4)\n",
        "\n",
        "    # state = 1 and a in (1, 2, 3)\n",
        "\n",
        "    # state = 2 and a in (0, 1, 2)\n",
        "\n",
        "    # state = 3 and a in (0, 1)\n",
        "    return V_new"
      ],
      "metadata": {
        "id": "o_XiHp48lMW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterations\n",
        "for it in range(100):\n",
        "    V = valueIterationUpdate(V, beta)\n",
        "    print(f\"Value after iteration {it+1}:\", {k: round(v, 2) for k, v in V.items()})"
      ],
      "metadata": {
        "id": "MoXq7eqNpZ2E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}